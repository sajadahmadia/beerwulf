# Answers
Dear Team,
Thanks for your instructions and sending the interview case. In this file, I will explain my answers to your questions in the readme file.

## Introduction: 
For this project, I used a tech stack of dbt and postges. I chose dbt because of its capablities that let us bring best practicies in software engineering to the world of sql scripts. It comes in with extendable modules, yml config files, built-in CI/CD capablities, data quality tests, and has an open source version. 
The reason that I chose postgres was because it is an open source database (as required by the case) and also we can create multiple schemas under one database (I used this feature for implementing the medallion architecture of my transformation logic).
- To get a visual picture of the project, you can use the 3 pictures provided in the `docs` directory. 

The instructions on how to run the project are given in the `HOW_TO_RUN.md` file. 

## Tech Stack:
* database: Postgressql (via Docker)
* Transformation: DBT 

## Answers to the questions:
Below, you can find the answers and instructions regarding my implementation of the project:

2. The final fact-dimension modeling scripts are provided in the beerwulf_dbt/models directory. 
    - I followed a Medallion architecture (bronze-silver-gold) for better structuring the project. The scripts for each layer is located at its corresponding folder.
    - To run the project, please visit the HOW_TO_RUN.md file. There, you can find detailed instructions on how to run this project.
    - Load scripts: The load scripts are located in the `initdb` directory. 
        * initdb/00-create-schemas.sql: creates schemas 
        * initdb/01-ingestion_zone_ddl.sql: ddl scripts, with relaxed (loosened) schema, to make the ingestion process more resilient, allowing raw data to be loaded even when it doesnâ€™t fully conform to strict types or constraints.
        * notice that based on the HOW_TO_RUN.md, I've made some shell scripts (in the `shell_scripts` directory) that run the mentioned load scripts automatically. scripts are re-runable, so you can run it as many times as you want with affecting the final results.
        * ERD file of the final model is provided at this path `beerwulf_dbt/assets/gold_schema_erd.png`. It will also render in the first page of the dbt documentation server when you run it. ![star schema](/assets/gold_schema_erd.png) 

    **Extra** point: 
    - define a classification: I used percentiles classification (33 and 67 as bounderies) based on customers' account balance. It's provided as a new field in the gold.dim_customer('customer_account_balance_segment')
    - add revenue per line item: I created a calculated field for it using this formula: revenue = extended_price * (1 - discount). You can find this column in silver.lineitem('revenue')

3. Using Azure stack, describe how you can schedule this process to run multiple times per day.
    - I assume that our target database is located on Azure stack. like a dedicated sql pool or azuer sql instance. first, we need to change our profile.yml file to match the credentials of the target database. second, to schedule running the tasks, we have 2 options: 
        1. Job Scheduler using DBT Cloud: Currently, We are using dbt core as an open source tool so that anyone can run it on his/her local machine. At production level, I recommend using dbt cloud which comes with an embeded task scheulder (also monitoring and CI/CD support) to run that pipeline based on any type of scheduling.
        2. Job Scheduler using Azure Data Factory: First, we need to containerize the dbt root folder(in my project, `beerwulf_dbt` directory). so we create a docker image out of it. We include `dbt run` command as the default command of the docker image. To store our Docker image on Azure, we push it to Azure Container Registry (ACR). ADF then uses Azure Container Instances to run the container from this image. Then, we need an ADF resource, with a **custom activity** that runs dbt commands. This activity, uses the ACR as a linked service, connects to it, and runs the required commands. for scheudling, we can use a **schedule trigger** with a recurrene of every 4 (as an example) hours. I am ready to discuss the details of this approach in our in-person meeting. 

        **Extra** point: 
        - What would you do to cater for data arriving in random order? This question and the next one are pointing the concept of Idempotent pipelines (Idempotency). For that, we need to use UPSERT(also called MERGE) operations. We need to have/use a business_key (unique key, usually generated by the producer) and an ingestion timestamp. The ingestion timestamp plays the role of **watermark column**, a concept that I learned when doing tasks with Azuer Stream Analytics (timestamp by column_name) to get ready for the DP203 certification. As an example, considering the code snippet below, if a row arrives with an old order_date but a fresh ingestion_timestamp, the merge logic knows to update that historical record and don't ignore it.
        ```
        {{ config(
            materialized='incremental',
            unique_key='order_key',
            incremental_strategy='merge'
        ) }}

        SELECT 
            *,
            CURRENT_TIMESTAMP as processed_at
        FROM {{ source('landing_zone','orders') }}
        {% if is_incremental() %}
        -- 3-day lookback window for late arrived data
        WHERE order_date >= (SELECT MAX(order_date) - INTERVAL '3' DAY FROM {{ this }})
        {% endif %}
        ```
        - What about if the data comes from a stream, and arrives at random times? The answer to this question, highly depends on our SLA. If we have a freshness SLA of every 30 minutes, or even 5 minutes, we need to have a landing_zone to continuously receive data with event timestamp. on each run of our (micro-)batch pipeline, we find the max event timestamp (imagine it's our watermark column). we define a look back window, like 2 hours. anything in this window(max of event time stamp and -2 horus), will be picked up and merged (processed), anything older than this window, won't. On the other hand, if our SLA is near real time, then we should change our stack and use a resource like Azure Stream Analytics that gives us a rich set of Windowing functions (like tumbling window, hopping winow, session window) to support such use cases that arise at that level. 

4. Describe how you would deploy your code to production, and allow for future maitenance? It depends on our chosen stack:
    1. If we are using DBT Cloud, we can have 2 different environments (dev, production). DBT connects to our remote github, each branch of our repo can be mapped to one of the mentioned environments. also, each environment connects to a specific schema under the current database. Then, we need to define the jobs:
    - **Dev job**: this job triggers on commits to dev branch, runs `dbt build --select state:modified+` to test only changed models
    - **CI job**: it gets triggered on pull requests, runs slim builds for faster feedback (slim build: run for only the selected models)
    - **Prod job**: it triggers on merges to the main branch, runs full `dbt build`, `but run` and `dbt test` commands. 
    for maintanance, we can rollback if needed using git and monitor for failed jobs via the built-in dbt monitoring that can send us messages.Also, we can use the dbt documentaion genration server that creates documentations from yml files automatically. 
    2. If we want to use Azure stack, the same philosphy applies. Just the tools are different: As I mentioned in question 3, we containerize our dbt project. also, tag each Docker image version (e.g., v1.2.3) for version control and rollback if needed. Then, we use AzureDevOps with a pipeline that gets triggered on our remote github commits and events:
        - Dev pipeline: it gets triggered when we push to the dev branch. builds a new docker image of our dev branch code base, pushes the new image into ACR, and updates our dev ADF piepline to use the new image. The dev pipeline runs `dbt build` that includes both model creation and testing.
        - CI pipeline: triggers on pull request and runs `dbt run --select state:modified+` and `dbt test` to validate changes before merge. 
        - Production pipeline: it triggers when a merge happens to the main branch. it builds the production docker image, tags it, pushes to ACR, and updates prod-ADF parameters with the new image tag that will be used on the next scheduled run
        - For maintanace, we can rollback to previous image tags if needed and monitor running the image via azure monitor or adf pipeline runs to set alerts.
        

5. Answers to the business questions using the created fact-dimension model:
    - The sql scripts for this part are located at `beerwulf_dbt/models/marts`. 

    a. What are the bottom 3 nations in terms of revenue?
        - script: `beerwulf_dbt/models/marts/bottom_3_nations_by_revenue.sql`
        - results: 

        | nation_name    | total_revenue |
        |----------------|---------------|
        | FRANCE         | 54431694      |
        | CHINA          | 65868584      |
        | UNITED STATES  | 65951276      |


    b. From the top 3 nations, what is the most common shipping mode?
        - script: `beerwulf_dbt/models/marts/top_3_nations_shipping_mode.sql`
        - results: 

        | top_shipment_method | shipment_count |
        |---------------------|----------------|
        | MAIL                | 1326           |


    c. What are the top 5 selling months? calculated based on total revenue
        - script: `beerwulf_dbt/models/marts/top_5_selling_months.sql`
        - results: 

        | year | month | month_name | monthly_revenue | order_count |
        |------|-------|------------|-----------------|-------------|
        | 1993 | 10    | October    | 31176188        | 201         |
        | 1993 | 12    | December   | 31122508        | 216         |
        | 1992 | 1     | January    | 30878747        | 203         |
        | 1996 | 8     | August     | 30497241        | 210         |
        | 1995 | 12    | December   | 30439158        | 217         |

    d. Who are the top customer(s) in terms of either revenue or quantity?
        - script: `beerwulf_dbt/models/marts/top_customers_revenue_quantity.sql`
        - results: 

    | customer_key | customer_name        | total_revenue | total_line_item_quantity | total_order_count |
    |-------------:|:---------------------|--------------:|-------------------------:|------------------:|
    |         1489 | Customer#000001489   |       5457271 |                     3868 |                29 |
    |          214 | Customer#000000214   |       4742500 |                     3369 |                25 |
    |           73 | Customer#000000073   |       4714756 |                     3384 |                30 |
    |         1396 | Customer#000001396   |       4678180 |                     3408 |                28 |
    |         1246 | Customer#000001246   |       4676316 |                     3226 |                27 |
    |          643 | Customer#000000643   |       4585648 |                     3201 |                32 |
    |         1150 | Customer#000001150   |       4557905 |                     3181 |                28 |
    |         1318 | Customer#000001318   |       4545914 |                     3229 |                29 |
    |          898 | Customer#000000898   |       4527601 |                     3309 |                32 |
    |          943 | Customer#000000943   |       4513235 |                     3229 |                29 |

    e.Compare the sales revenue on a financial year-to-year (01 July to 30 June) basis.
    - script: `beerwulf_dbt/models/marts/fiscal_revenue_comparison.sql`
    - results: 

    | fiscal_year | current_year_revenue | previous_year_revenue | revenue_growth_pct |
    |-----------:|---------------------:|----------------------:|-------------------:|
    |        1991 |           166760066 |                  NULL |              NULL |
    |        1992 |           316312476 |             166760066 |             89.68 |
    |        1993 |           342780431 |             316312476 |              8.37 |
    |        1994 |           317890996 |             342780431 |             -7.26 |
    |        1995 |           325880103 |             317890996 |              2.51 |
    |        1996 |           335094235 |             325880103 |              2.83 |
    |        1997 |           318227058 |             335094235 |             -5.03 |
    |        1998 |            29245339 |             318227058 |            -90.81 |
